{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8804868-26bd-4e4c-98c7-79adf67d9d2f",
   "metadata": {},
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501f8d3-c45a-41e7-a959-4a05dc406633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# %matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "from rank_bm25 import BM25Okapi, BM25L, BM25Plus\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import re\n",
    "\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "# import wandb\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "# wandb_logger = WandbLogger(name=\"preprocess\", project=\"DACON_236228\")\n",
    "\n",
    "parser = ArgumentParser(description=\"preprocess\")\n",
    "parser.add_argument('--text_pretrained_model', default=\"unixcoder-base\", type=str)\n",
    "parser.add_argument('--truncation_side', default='left', type=str) # right or left\n",
    "parser.add_argument('--bm25', default='bm25plus', type=str)\n",
    "parser.add_argument('--frac', default=0.01, type=float)\n",
    "parser.add_argument('--seed', default=826, type=int)\n",
    "parser.add_argument('--device', default=0, type=int)\n",
    "parser.add_argument('--num_workers', default=0, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "# wandb.config.update(args)\n",
    "\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "\n",
    "SEED = args.seed\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    pl.seed_everything(SEED)\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "idx = f\"{args.text_pretrained_model}_{args.bm25}\"\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a92f51-324b-4b99-9bf4-07adef5d568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.text_pretrained_model == \"unixcoder-base\": # 1024\n",
    "    text_pretrained_model = \"microsoft/unixcoder-base\"\n",
    "if args.text_pretrained_model == \"graphcodebert-base\": # 512\n",
    "    text_pretrained_model = \"microsoft/graphcodebert-base\"\n",
    "if args.text_pretrained_model == \"codebert-base\": # 512\n",
    "    text_pretrained_model = \"microsoft/codebert-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_pretrained_model)\n",
    "tokenizer.truncation_side = args.truncation_side"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d7fb1f6-bf7d-4151-b66e-c731ea4bb83d",
   "metadata": {},
   "source": [
    "## preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360ae394-e875-4064-aa04-51b44d602329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cpp_code(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        return file.read()\n",
    "\n",
    "train_code_paths = glob('data/train_code/*/*.cpp')\n",
    "\n",
    "len(train_code_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e214c78d-91e6-4b88-8c29-1dea74af8229",
   "metadata": {},
   "outputs": [],
   "source": [
    "read_cpp_code(train_code_paths[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4923a245-db1e-4144-ab1b-d411c2b86f0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "val = pd.read_csv(\"data/sample_train.csv\")\n",
    "val.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e846980-5d8f-46e4-a082-e007519d90ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' 데이터 클리닝 '''\n",
    "def clean_data(script, data_type=\"dir\"):\n",
    "    if data_type == \"dir\":\n",
    "        with open(script, 'r', encoding='utf-8') as file:\n",
    "            lines = file.readlines()\n",
    "            preproc_lines = []\n",
    "            in_multiline_comment = False\n",
    "            for line in lines:\n",
    "                if line.startswith('#include'): # #include로 시작하는 행 제거\n",
    "                    continue\n",
    "                line = line.strip().replace('\\t', '').split('//')[0].strip() # 개행문자 제거, 주석 제거\n",
    "                line = re.sub(' +', ' ', line) # 개행문자 제거\n",
    "                if line == '': # 전처리 후 빈 라인은 skip\n",
    "                    continue\n",
    "                # 여러 줄 주석 시작\n",
    "                if '/*' in line:\n",
    "                    in_multiline_comment = True\n",
    "                # 여러 줄 주석 안에 있는 내용은 무시\n",
    "                if not in_multiline_comment:\n",
    "                    preproc_lines.append(line)\n",
    "                # 여러 줄 주석 종료\n",
    "                if '*/' in line:\n",
    "                    in_multiline_comment = False\n",
    "\n",
    "    elif data_type == \"file\":\n",
    "        lines = script.split('\\n')\n",
    "        preproc_lines = []\n",
    "        in_multiline_comment = False\n",
    "        for line in lines:\n",
    "            if line.startswith('#include'): # #include로 시작하는 행 제거\n",
    "                continue\n",
    "            line = line.strip().replace('\\t', '').split('//')[0].strip() # 개행문자 제거, 주석 제거\n",
    "            line = re.sub(' +', ' ', line) # 개행문자 제거\n",
    "            if line == '': # 전처리 후 빈 라인은 skip\n",
    "                continue\n",
    "            # 여러 줄 주석 시작\n",
    "            if '/*' in line:\n",
    "                in_multiline_comment = True\n",
    "            # 여러 줄 주석 안에 있는 내용은 무시\n",
    "            if not in_multiline_comment:\n",
    "                preproc_lines.append(line)\n",
    "            # 여러 줄 주석 종료\n",
    "            if '*/' in line:\n",
    "                in_multiline_comment = False\n",
    "\n",
    "    processed_script = ' '.join(preproc_lines) # 개행 문자로 합침\n",
    "    # processed_script = '\\n'.join(preproc_lines) # 개행 문자로 합침\n",
    "    return processed_script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc09884-b3cc-4ffd-b058-fb9a22cf5b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_data(read_cpp_code(train_code_paths[0]), data_type=\"file\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe8718e-ee9c-4ef7-a669-f819ec7e1616",
   "metadata": {},
   "outputs": [],
   "source": [
    "''' positive, negative 페어 생성 함수 '''\n",
    "def get_pairs(input_df, tokenizer):\n",
    "    codes = input_df['code'].to_list()\n",
    "    problems = input_df['problem_num'].unique().tolist()\n",
    "    problems.sort()\n",
    "\n",
    "    tokenized_corpus = [tokenizer.tokenize(code) for code in codes]\n",
    "    if args.bm25 == \"bm25ok\":\n",
    "        bm25 = BM25Okapi(tokenized_corpus)\n",
    "    if args.bm25 == \"bm25l\":\n",
    "        bm25 = BM25L(tokenized_corpus)\n",
    "    if args.bm25 == \"bm25plus\":\n",
    "        bm25 = BM25Plus(tokenized_corpus)\n",
    "\n",
    "    total_positive_pairs = []\n",
    "    total_negative_pairs = []\n",
    "\n",
    "    for problem in tqdm(problems):\n",
    "        solution_codes = input_df[input_df['problem_num'] == problem]['code']\n",
    "        positive_pairs = list(combinations(solution_codes.to_list(),2))\n",
    "\n",
    "        solution_codes_indices = solution_codes.index.to_list()\n",
    "        negative_pairs = []\n",
    "\n",
    "        first_tokenized_code = tokenizer.tokenize(positive_pairs[0][0])\n",
    "        negative_code_scores = bm25.get_scores(first_tokenized_code)\n",
    "        negative_code_ranking = negative_code_scores.argsort()[::-1] # 내림차순\n",
    "        ranking_idx = 0\n",
    "\n",
    "        for solution_code in solution_codes:\n",
    "            negative_solutions = []\n",
    "            while len(negative_solutions) < len(positive_pairs) // len(solution_codes):\n",
    "                high_score_idx = negative_code_ranking[ranking_idx]\n",
    "\n",
    "                if high_score_idx not in solution_codes_indices:\n",
    "                    negative_solutions.append(input_df['code'].iloc[high_score_idx])\n",
    "                ranking_idx += 1\n",
    "\n",
    "            for negative_solution in negative_solutions:\n",
    "                negative_pairs.append((solution_code, negative_solution))\n",
    "\n",
    "        total_positive_pairs.extend(positive_pairs)\n",
    "        total_negative_pairs.extend(negative_pairs)\n",
    "\n",
    "    pos_code1 = list(map(lambda x:x[0],total_positive_pairs))\n",
    "    pos_code2 = list(map(lambda x:x[1],total_positive_pairs))\n",
    "\n",
    "    neg_code1 = list(map(lambda x:x[0],total_negative_pairs))\n",
    "    neg_code2 = list(map(lambda x:x[1],total_negative_pairs))\n",
    "\n",
    "    pos_label = [1]*len(pos_code1)\n",
    "    neg_label = [0]*len(neg_code1)\n",
    "\n",
    "    pos_code1.extend(neg_code1)\n",
    "    total_code1 = pos_code1\n",
    "    pos_code2.extend(neg_code2)\n",
    "    total_code2 = pos_code2\n",
    "    pos_label.extend(neg_label)\n",
    "    total_label = pos_label\n",
    "    \n",
    "    pair_data = pd.DataFrame(data={\n",
    "        'code1':total_code1,\n",
    "        'code2':total_code2,\n",
    "        'similar':total_label\n",
    "    })\n",
    "    \n",
    "    return pair_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6b6458-0891-495a-8f9c-50fab1a13ef0",
   "metadata": {},
   "source": [
    "## data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8ce41c-2cae-497c-9969-194a6efcc221",
   "metadata": {},
   "outputs": [],
   "source": [
    "code_folder = \"data/train_code\"\n",
    "problem_folders = os.listdir(code_folder)\n",
    "processed_scripts = []\n",
    "problem_nums = []\n",
    "\n",
    "for problem_folder in tqdm(problem_folders):\n",
    "    scripts = os.listdir(os.path.join(code_folder, problem_folder))\n",
    "    problem_num = scripts[0].split('_')[0]\n",
    "    for script in scripts:\n",
    "        script_file = os.path.join(code_folder, problem_folder, script)\n",
    "        processed_script = clean_data(script_file, data_type=\"dir\")\n",
    "        processed_scripts.append(processed_script)\n",
    "    problem_nums.extend([problem_num] * len(scripts))\n",
    "    \n",
    "pp_train_df = pd.DataFrame(\n",
    "    data={'code': processed_scripts, 'problem_num': problem_nums}\n",
    ")\n",
    "\n",
    "pp_train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6a21c3-bff6-4370-a12f-6baec1532c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_train_bm25 = get_pairs(pp_train_df, tokenizer)\n",
    "\n",
    "plength = len(pp_train_bm25) // 10\n",
    "for i in tqdm(range(10)):\n",
    "    pp_train_df = pp_train_bm25.iloc[i*plength:(i+1)*plength]\n",
    "    pp_train_df.to_parquet(f'pp_train_{idx}_{i}.parquet', engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef170ab-4078-4022-a64a-1db50b06c180",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "code1 = test_df['code1'].values\n",
    "code2 = test_df['code2'].values\n",
    "processed_code1 = []\n",
    "processed_code2 = []\n",
    "for i in tqdm(range(len(code1))):\n",
    "    processed_c1 = clean_data(code1[i], data_type=\"file\")\n",
    "    processed_c2 = clean_data(code2[i], data_type=\"file\")\n",
    "    processed_code1.append(processed_c1)\n",
    "    processed_code2.append(processed_c2)\n",
    "    \n",
    "pp_test_df = pd.DataFrame(\n",
    "    list(zip(processed_code1, processed_code2)), columns=[\"code1\", \"code2\"]\n",
    ")\n",
    "    \n",
    "pp_test_df.to_parquet(f'pp_test_{idx}.parquet', engine='pyarrow', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e78cc957-abf5-4802-bb6e-88e62a8f2dc4",
   "metadata": {},
   "source": [
    "## sample generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d5c45c-faba-49da-a320-e50737414c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "for i in range(10):\n",
    "    temp_df = pd.read_parquet(f'pp_train_{idx}_{i}.parquet', engine='pyarrow')\n",
    "    df = pd.concat([df, temp_df])\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5307237a-14fa-4ded-84b5-7254e4b79642",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_df = df.sample(frac=args.frac, random_state=SEED).reset_index(drop=True)\n",
    "sample_df = sample_df[(sample_df[\"code1\"]!=\"\") & (sample_df[\"code2\"]!=\"\")]\n",
    "sample_df.to_csv(f\"pp_train_{idx}_frac{args.frac}.csv\", index=False)\n",
    "\n",
    "len(sample_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6604ea-3499-406a-9084-6b8e45f9c7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
