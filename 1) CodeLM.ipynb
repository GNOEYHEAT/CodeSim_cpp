{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8804868-26bd-4e4c-98c7-79adf67d9d2f",
   "metadata": {},
   "source": [
    "# CodeLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e501f8d3-c45a-41e7-a959-4a05dc406633",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "# %matplotlib inline\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "transformers.logging.set_verbosity_error()\n",
    "\n",
    "# from rank_bm25 import BM25Okapi, BM25L, BM25Plus\n",
    "# from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from itertools import combinations\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "# import wandb\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "\n",
    "# wandb.init(project=\"DACON_236228\", name=\"CodeLM\")\n",
    "# wandb_logger = WandbLogger(project=\"DACON_236228\", name=\"CodeLM\")\n",
    "\n",
    "parser = ArgumentParser(description=\"CodeLM\")\n",
    "\n",
    "parser.add_argument('--text_pretrained_model', default=\"graphcodebert-base\", type=str)\n",
    "parser.add_argument('--text_len', default=512, type=int)\n",
    "parser.add_argument('--truncation_side', default='left', type=str) # right or left\n",
    "parser.add_argument('--optimizer', default=\"adamw\", type=str)\n",
    "parser.add_argument('--learning_rate', default=0.00003, type=float)\n",
    "parser.add_argument('--scheduler', default=\"none\", type=str)\n",
    "parser.add_argument('--batch_size', default=32, type=int)\n",
    "parser.add_argument('--epochs', default=10, type=int)\n",
    "parser.add_argument('--cv', default=5, type=int)\n",
    "parser.add_argument('--seed', default=826, type=int)\n",
    "parser.add_argument('--mixed_precision', default=16, type=int)\n",
    "parser.add_argument('--device', nargs='+', default=[0], type=int)\n",
    "parser.add_argument('--num_workers', default=0, type=int)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "# wandb.config.update(args)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "text_len = args.text_len\n",
    "BATCH_SIZE = args.batch_size\n",
    "EPOCHS = args.epochs\n",
    "CV = args.cv\n",
    "SEED = args.seed\n",
    "\n",
    "def set_seeds(seed=SEED):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    pl.seed_everything(SEED)\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "idx = f\"{args.text_pretrained_model}\"\n",
    "idx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d7e2128-ba8c-4eda-b280-989bd7af44e2",
   "metadata": {},
   "source": [
    "## pretrain.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86a92f51-324b-4b99-9bf4-07adef5d568a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.text_pretrained_model == \"unixcoder-base\": # 1024\n",
    "    txt_model_name = \"microsoft/unixcoder-base\"\n",
    "    latent_dim = 768\n",
    "if args.text_pretrained_model == \"graphcodebert-base\": # 512\n",
    "    txt_model_name = \"microsoft/graphcodebert-base\"\n",
    "    latent_dim = 768\n",
    "if args.text_pretrained_model == \"codebert-base\": # 512\n",
    "    txt_model_name = \"microsoft/codebert-base\"\n",
    "    latent_dim = 768\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(txt_model_name)\n",
    "\n",
    "model = AutoModel.from_pretrained(txt_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc227163-258a-48d6-8dba-ff0c85e6e7e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/sample_train.csv\")\n",
    "# df = pd.read_csv(\"data/pp_train_graphcodebert-base_bm25plus_frac0.02.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a3b87bc-e040-4530-9150-792bf3de987c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv(\"data/test.csv\")\n",
    "# test_df = pd.read_parquet('data/pp_test_graphcodebert-base_bm25plus.parquet', engine='pyarrow')\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d15dd81-4b02-41a7-8c43-30663b09a217",
   "metadata": {},
   "source": [
    "## data_loader.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94d35b7-5881-4945-897f-3dc210dbd147",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, df, is_test=False):\n",
    "        self.df = df\n",
    "        self.is_test = is_test\n",
    "        self.tokenizer = tokenizer\n",
    "        if args.truncation_side == \"left\":\n",
    "            self.tokenizer.truncation_side = 'left'\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            row['code1'], row['code2'],\n",
    "            add_special_tokens=True,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            max_length=text_len,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "\n",
    "        for k,v in encoding.items():\n",
    "            encoding[k] = v.squeeze()\n",
    "\n",
    "        if not self.is_test:\n",
    "            labels = torch.tensor(row[\"similar\"], dtype=torch.float)\n",
    "            return encoding, labels\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e1434d-fa7b-4434-926a-54258186f505",
   "metadata": {},
   "source": [
    "## model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c65b79e-871b-4c5e-991d-ed65376ff6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.txt_model = AutoModel.from_pretrained(txt_model_name)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        txt_side = self.txt_model(**inputs)\n",
    "        txt_feature = txt_side.last_hidden_state[:, 0, :]\n",
    "        outputs = self.classifier(txt_feature)\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6d0980-2c33-433b-bcec-b01e75120bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(pl.LightningModule):\n",
    "    def __init__(self, backbone, args):\n",
    "        super().__init__()\n",
    "        self.backbone = backbone\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = self.backbone(x)\n",
    "        return outputs\n",
    "\n",
    "    def step(self, batch):\n",
    "        x = batch[0]\n",
    "        y = batch[1]\n",
    "        y_hat = self.forward(x)\n",
    "        loss = nn.BCEWithLogitsLoss()(y_hat.squeeze(), y)\n",
    "        return loss, y, y_hat\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        y_pred = (y_hat > 0).float().squeeze()\n",
    "        acc = (y_pred == y).float().mean()\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"train_acc\", acc, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "        y_pred = (y_hat > 0).float().squeeze()\n",
    "        acc = (y_pred == y).float().mean()\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log(\"val_acc\", acc, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        loss, y, y_hat = self.step(batch)\n",
    "\n",
    "    def predict_step(self, batch, batch_idx, dataloader_idx=0):\n",
    "        y_hat = self.forward(batch)\n",
    "        return y_hat\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        if args.optimizer == \"sgd\":\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=args.learning_rate, momentum=0.9)\n",
    "        if args.optimizer == \"adam\":\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=args.learning_rate)\n",
    "        if args.optimizer == \"adamw\":\n",
    "            optimizer = torch.optim.AdamW(self.parameters(), lr=args.learning_rate)\n",
    "        \n",
    "        if args.scheduler == \"none\":\n",
    "            return optimizer\n",
    "        if args.scheduler == \"cosine\":\n",
    "            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer=optimizer,\n",
    "                T_max=args.epochs//2,\n",
    "                eta_min=args.learning_rate//10,\n",
    "            )\n",
    "            return [optimizer], [scheduler]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2abecdf6-8b04-491e-9576-2b07e1e95254",
   "metadata": {},
   "source": [
    "## train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "990bf9ea-dc82-46d8-a825-6bb2f0f04c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "## preprocessing.py\n",
    "\n",
    "val_acc_list = []\n",
    "preds_list = []\n",
    "\n",
    "skf = StratifiedKFold(n_splits=CV, shuffle=True, random_state=SEED)\n",
    "\n",
    "for i, (train_index, val_index) in enumerate(skf.split(df, df['similar'])):\n",
    "\n",
    "    train_df = df.iloc[train_index]\n",
    "    val_df = df.iloc[val_index]\n",
    "\n",
    "## data_loaders.py\n",
    "\n",
    "    train_ds = TextDataset(train_df, False)\n",
    "    val_ds = TextDataset(val_df, False)\n",
    "    test_ds = TextDataset(test_df, True)\n",
    "\n",
    "    train_dataloader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=args.num_workers)\n",
    "    val_dataloader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=args.num_workers)\n",
    "    test_dataloader = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=args.num_workers)\n",
    "\n",
    "## train.py\n",
    "\n",
    "    model = TextClassifier(TextModel(), args)\n",
    "\n",
    "    callbacks = [\n",
    "        # pl.callbacks.EarlyStopping(\n",
    "        #     monitor=\"val_acc\", patience=3, mode=\"max\"\n",
    "        # ),\n",
    "        pl.callbacks.ModelCheckpoint(\n",
    "            dirpath=\"saved/\", filename=f\"{idx}_{i}\",\n",
    "            monitor=\"val_acc\", mode=\"max\"\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=EPOCHS, accelerator=\"auto\", callbacks=callbacks,\n",
    "        precision=args.mixed_precision, #logger=wandb_logger,\n",
    "        devices=args.device, #strategy='ddp_find_unused_parameters_true'\n",
    "    )\n",
    "\n",
    "    trainer.fit(model, train_dataloader, val_dataloader)\n",
    "    \n",
    "    ckpt = torch.load(f\"saved/{idx}_{i}.ckpt\")\n",
    "    model.load_state_dict(ckpt['state_dict'])\n",
    "\n",
    "## test.py\n",
    "\n",
    "    eval_dict = trainer.validate(model, dataloaders=val_dataloader)[0]\n",
    "    val_acc_list.append(eval_dict[\"val_acc\"])\n",
    "    \n",
    "    preds = trainer.predict(model, dataloaders=test_dataloader)\n",
    "    preds_list.append(np.vstack(preds))\n",
    "\n",
    "    break\n",
    "    \n",
    "val_acc_mean = np.mean(val_acc_list)\n",
    "\n",
    "print(f\"val_acc_mean: {val_acc_mean}\")\n",
    "# wandb.log({'val_acc_mean': val_acc_mean})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84e6e86-4184-4668-b040-53d614976217",
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.mean(preds_list, axis=0)\n",
    "\n",
    "preds.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c54fef3-d13c-4a42-a293-de406be9af2a",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9066daf-7453-4852-8a7d-41d107ccbe3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('data/sample_submission.csv')\n",
    "submission['similar'] = np.where(preds>0, 1, 0)\n",
    "submission.to_csv(f'{idx}.csv', index=False)\n",
    "\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6604ea-3499-406a-9084-6b8e45f9c7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
